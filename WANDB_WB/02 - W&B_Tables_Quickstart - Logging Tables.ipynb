{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_AbkduPX21V"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/datasets-predictions/W&B_Tables_Quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{tables_quickstart} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fNL_hyOX21Y"
   },
   "source": [
    "<img src=\"http://wandb.me/logo-im-png\" width=\"300\" alt=\"Weights & Biases\" />\n",
    "\n",
    "<!--- @wandbcode{tables_quickstart} -->\n",
    "\n",
    "\n",
    "# View & analyze model predictions during training\n",
    "\n",
    "This quickstart guide covers how to track, visualize, and compare model predictions over the course of training, using PyTorch on MNIST data.\n",
    "\n",
    "With [W&B Tables](https://docs.wandb.com/datasets-and-predictions):\n",
    "1. Log metrics, images, text, etc. to a `wandb.Table()` during model training or evaluation\n",
    "2. View, sort, filter, group, join, interactively query, and explore these tables\n",
    "3. Compare model predictions or results: dynamically across specific images, hyperparameters/model versions, or time steps.\n",
    "\n",
    "# Examples\n",
    "## Compare predicted scores for specific images\n",
    "\n",
    "[Live example: compare predictions after 1 vs 5 epochs of training →](https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU#compare-predictions-after-1-vs-5-epochs)\n",
    "<img src=\"https://i.imgur.com/NMme6Qj.png\" alt=\"1 epoch vs 5 epochs of training\"/>\n",
    "The histograms compare per-class scores between the two models. The top green bar in each histogram represents model \"CNN-2, 1 epoch\" (id 0), which only trained for 1 epoch. The bottom purple bar represents model \"CNN-2, 5 epochs\" (id 1), which trained for 5 epochs. The images are filtered to cases where the models disagree. For example, in the first row, the \"4\" gets high scores across all the possible digits after 1 epoch, but after 5 epochs it scores highest on the correct label and very low on the rest.\n",
    "\n",
    "## Focus on top errors over time\n",
    "[Live example →](https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU#top-errors-over-time)\n",
    "\n",
    "See incorrect predictions (filter to rows where \"guess\" != \"truth\") on the full test data. Note that there are 229 wrong guesses after 1 training epoch, but only 98 after 5 epochs.\n",
    "<img src=\"https://i.imgur.com/7g8nodn.png\" alt=\"side by side, 1 vs 5 epochs of training\"/>\n",
    "\n",
    "## Compare model performance and find patterns\n",
    "\n",
    "[See full detail in a live example →](https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU#false-positives-grouped-by-guess)\n",
    "\n",
    "Filter out correct answers, then group by the guess to see examples of misclassified images and the underlying distribution of true labels—for two models side-by-side. A model variant with 2X the layer sizes and learning rate is on the left, and the baseline is on the right. Note that the baseline makes slightly more mistakes for each guessed class.\n",
    "<img src=\"https://i.imgur.com/i5PP9AE.png\" alt=\"grouped errors for baseline vs double variant\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipHUcWMuX21Z"
   },
   "source": [
    "# Sign up or login\n",
    "\n",
    "[Sign up or login](https://wandb.ai/login) to W&B to see and interact with your experiments in the browser.\n",
    "\n",
    "In this example we're using Google Colab as a convenient hosted environment, but you can run your own training scripts from anywhere and visualize metrics with W&B's experiment tracking tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tAOggICkX21Z"
   },
   "outputs": [],
   "source": [
    "# !pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAdCP_e0X21a"
   },
   "source": [
    "log to your account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MVi4wBMzX21a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "\n",
    "WANDB_PROJECT = \"mnist-viz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jblAcZ4SX21b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: arun5mary (arun5mary-tredence). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDwbCISPX21b"
   },
   "source": [
    "# 0. Setup\n",
    "\n",
    "Install dependencies, download MNIST, and create train and test datasets using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fNFH8MZGX21b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# create train and test dataloaders\n",
    "def get_dataloader(is_train, batch_size, slice=5):\n",
    "    \"Get a training dataloader\"\n",
    "    ds = torchvision.datasets.MNIST(root=\".\", train=is_train, transform=T.ToTensor(), download=True)\n",
    "    loader = torch.utils.data.DataLoader(dataset=ds,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True if is_train else False,\n",
    "                                         pin_memory=True, num_workers=2)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53ns4VgaX21b"
   },
   "source": [
    "# 1. Define the model and training schedule\n",
    "\n",
    "* Set the number of epochs to run, where each epoch consists of a training step and a validation (test) step. Optionally configure the amount of data to log per test step. Here the number of batches and number of images per batch to visualize are set low to simplify the demo.\n",
    "* Define a simple convolutional neural net (following [pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial) code).\n",
    "* Load in train and test sets using PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dVS1qxpiX21b"
   },
   "outputs": [],
   "source": [
    "# Number of epochs to run\n",
    "# Each epoch includes a training step and a test step, so this sets\n",
    "# the number of tables of test predictions to log\n",
    "EPOCHS = 5\n",
    "\n",
    "# Number of batches to log from the test data for each test step\n",
    "# (default set low to simplify demo)\n",
    "NUM_BATCHES_TO_LOG = 10 #79\n",
    "\n",
    "# Number of images to log per test batch\n",
    "# (default set low to simplify demo)\n",
    "NUM_IMAGES_PER_BATCH = 32 #128\n",
    "\n",
    "# training configuration and hyperparameters\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "L1_SIZE = 32\n",
    "L2_SIZE = 64\n",
    "# changing this may require changing the shape of adjacent layers\n",
    "CONV_KERNEL_SIZE = 5\n",
    "\n",
    "# define a two-layer convolutional neural network\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, L1_SIZE, CONV_KERNEL_SIZE, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(L1_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(L1_SIZE, L2_SIZE, CONV_KERNEL_SIZE, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(L2_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*L2_SIZE, NUM_CLASSES)\n",
    "        self.softmax = nn.Softmax(NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # uncomment to see the shape of a given layer:\n",
    "        #print(\"x: \", x.size())\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "train_loader = get_dataloader(is_train=True, batch_size=BATCH_SIZE)\n",
    "test_loader = get_dataloader(is_train=False, batch_size=2*BATCH_SIZE)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEYHDM0VX21c"
   },
   "source": [
    "# 2. Run training and log test predictions\n",
    "\n",
    "For every epoch, run a training step and a test step. For each test step, create a wandb.Table() in which to store test predictions. These can be visualized, dynamically queried, and compared side by side in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Y1QzZyUAX21c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be74b7b4354243e9b202119d7c19341c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888886984852, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\arunp\\Projects\\LLMOPS\\WANDB\\wandb\\run-20250116_193351-g1e95qqf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arun5mary-tredence/table-quickstart/runs/g1e95qqf' target=\"_blank\">celestial-frog-2</a></strong> to <a href='https://wandb.ai/arun5mary-tredence/table-quickstart' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arun5mary-tredence/table-quickstart' target=\"_blank\">https://wandb.ai/arun5mary-tredence/table-quickstart</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arun5mary-tredence/table-quickstart/runs/g1e95qqf' target=\"_blank\">https://wandb.ai/arun5mary-tredence/table-quickstart/runs/g1e95qqf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/1875], Loss: 0.3103\n",
      "Epoch [1/5], Step [200/1875], Loss: 0.0984\n",
      "Epoch [1/5], Step [300/1875], Loss: 0.0693\n",
      "Epoch [1/5], Step [400/1875], Loss: 0.3682\n",
      "Epoch [1/5], Step [500/1875], Loss: 0.2364\n",
      "Epoch [1/5], Step [600/1875], Loss: 0.1698\n",
      "Epoch [1/5], Step [700/1875], Loss: 0.0598\n",
      "Epoch [1/5], Step [800/1875], Loss: 0.0660\n",
      "Epoch [1/5], Step [900/1875], Loss: 0.0842\n",
      "Epoch [1/5], Step [1000/1875], Loss: 0.0252\n",
      "Epoch [1/5], Step [1100/1875], Loss: 0.0686\n",
      "Epoch [1/5], Step [1200/1875], Loss: 0.0070\n",
      "Epoch [1/5], Step [1300/1875], Loss: 0.2521\n",
      "Epoch [1/5], Step [1400/1875], Loss: 0.0351\n",
      "Epoch [1/5], Step [1500/1875], Loss: 0.0034\n",
      "Epoch [1/5], Step [1600/1875], Loss: 0.0063\n",
      "Epoch [1/5], Step [1700/1875], Loss: 0.0430\n",
      "Epoch [1/5], Step [1800/1875], Loss: 0.0022\n",
      "Test Accuracy of the model on the 10000 test images: 98.32 %\n",
      "Epoch [2/5], Step [100/1875], Loss: 0.0199\n",
      "Epoch [2/5], Step [200/1875], Loss: 0.0051\n",
      "Epoch [2/5], Step [300/1875], Loss: 0.0216\n",
      "Epoch [2/5], Step [400/1875], Loss: 0.0159\n",
      "Epoch [2/5], Step [500/1875], Loss: 0.0104\n",
      "Epoch [2/5], Step [600/1875], Loss: 0.0284\n",
      "Epoch [2/5], Step [700/1875], Loss: 0.1128\n",
      "Epoch [2/5], Step [800/1875], Loss: 0.0139\n",
      "Epoch [2/5], Step [900/1875], Loss: 0.0006\n",
      "Epoch [2/5], Step [1000/1875], Loss: 0.0265\n",
      "Epoch [2/5], Step [1100/1875], Loss: 0.0024\n",
      "Epoch [2/5], Step [1200/1875], Loss: 0.0059\n",
      "Epoch [2/5], Step [1300/1875], Loss: 0.0017\n",
      "Epoch [2/5], Step [1400/1875], Loss: 0.0042\n",
      "Epoch [2/5], Step [1500/1875], Loss: 0.1235\n",
      "Epoch [2/5], Step [1600/1875], Loss: 0.0073\n",
      "Epoch [2/5], Step [1700/1875], Loss: 0.0197\n",
      "Epoch [2/5], Step [1800/1875], Loss: 0.0154\n",
      "Test Accuracy of the model on the 10000 test images: 98.46 %\n",
      "Epoch [3/5], Step [100/1875], Loss: 0.0642\n",
      "Epoch [3/5], Step [200/1875], Loss: 0.0319\n",
      "Epoch [3/5], Step [300/1875], Loss: 0.0016\n",
      "Epoch [3/5], Step [400/1875], Loss: 0.0008\n",
      "Epoch [3/5], Step [500/1875], Loss: 0.0140\n",
      "Epoch [3/5], Step [600/1875], Loss: 0.0017\n",
      "Epoch [3/5], Step [700/1875], Loss: 0.0021\n",
      "Epoch [3/5], Step [800/1875], Loss: 0.0427\n",
      "Epoch [3/5], Step [900/1875], Loss: 0.0074\n",
      "Epoch [3/5], Step [1000/1875], Loss: 0.0273\n",
      "Epoch [3/5], Step [1100/1875], Loss: 0.0091\n",
      "Epoch [3/5], Step [1200/1875], Loss: 0.0160\n",
      "Epoch [3/5], Step [1300/1875], Loss: 0.0905\n",
      "Epoch [3/5], Step [1400/1875], Loss: 0.0069\n",
      "Epoch [3/5], Step [1500/1875], Loss: 0.0208\n",
      "Epoch [3/5], Step [1600/1875], Loss: 0.0107\n",
      "Epoch [3/5], Step [1700/1875], Loss: 0.0133\n",
      "Epoch [3/5], Step [1800/1875], Loss: 0.0014\n",
      "Test Accuracy of the model on the 10000 test images: 99.21 %\n",
      "Epoch [4/5], Step [100/1875], Loss: 0.2619\n",
      "Epoch [4/5], Step [200/1875], Loss: 0.0319\n",
      "Epoch [4/5], Step [300/1875], Loss: 0.0015\n",
      "Epoch [4/5], Step [400/1875], Loss: 0.1719\n",
      "Epoch [4/5], Step [500/1875], Loss: 0.1194\n",
      "Epoch [4/5], Step [600/1875], Loss: 0.0105\n",
      "Epoch [4/5], Step [700/1875], Loss: 0.0047\n",
      "Epoch [4/5], Step [800/1875], Loss: 0.0709\n",
      "Epoch [4/5], Step [900/1875], Loss: 0.0016\n",
      "Epoch [4/5], Step [1000/1875], Loss: 0.0017\n",
      "Epoch [4/5], Step [1100/1875], Loss: 0.0133\n",
      "Epoch [4/5], Step [1200/1875], Loss: 0.0041\n",
      "Epoch [4/5], Step [1300/1875], Loss: 0.0018\n",
      "Epoch [4/5], Step [1400/1875], Loss: 0.0003\n",
      "Epoch [4/5], Step [1500/1875], Loss: 0.0647\n",
      "Epoch [4/5], Step [1600/1875], Loss: 0.0004\n",
      "Epoch [4/5], Step [1700/1875], Loss: 0.0004\n",
      "Epoch [4/5], Step [1800/1875], Loss: 0.0121\n",
      "Test Accuracy of the model on the 10000 test images: 99.05 %\n",
      "Epoch [5/5], Step [100/1875], Loss: 0.0019\n",
      "Epoch [5/5], Step [200/1875], Loss: 0.0146\n",
      "Epoch [5/5], Step [300/1875], Loss: 0.0370\n",
      "Epoch [5/5], Step [400/1875], Loss: 0.0125\n",
      "Epoch [5/5], Step [500/1875], Loss: 0.0224\n",
      "Epoch [5/5], Step [600/1875], Loss: 0.0274\n",
      "Epoch [5/5], Step [700/1875], Loss: 0.0031\n",
      "Epoch [5/5], Step [800/1875], Loss: 0.0167\n",
      "Epoch [5/5], Step [900/1875], Loss: 0.0022\n",
      "Epoch [5/5], Step [1000/1875], Loss: 0.0001\n",
      "Epoch [5/5], Step [1100/1875], Loss: 0.0610\n",
      "Epoch [5/5], Step [1200/1875], Loss: 0.0094\n",
      "Epoch [5/5], Step [1300/1875], Loss: 0.0378\n",
      "Epoch [5/5], Step [1400/1875], Loss: 0.0116\n",
      "Epoch [5/5], Step [1500/1875], Loss: 0.0027\n",
      "Epoch [5/5], Step [1600/1875], Loss: 0.0074\n",
      "Epoch [5/5], Step [1700/1875], Loss: 0.0031\n",
      "Epoch [5/5], Step [1800/1875], Loss: 0.0003\n",
      "Test Accuracy of the model on the 10000 test images: 99.06 %\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▂█▇▇</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▂▃▂▃▄▆▂▁▁▁▂▂▂▁▄▁▁▂▂▃▂▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>99.06</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.1292</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">celestial-frog-2</strong> at: <a href='https://wandb.ai/arun5mary-tredence/table-quickstart/runs/g1e95qqf' target=\"_blank\">https://wandb.ai/arun5mary-tredence/table-quickstart/runs/g1e95qqf</a><br> View project at: <a href='https://wandb.ai/arun5mary-tredence/table-quickstart' target=\"_blank\">https://wandb.ai/arun5mary-tredence/table-quickstart</a><br>Synced 5 W&B file(s), 5 media file(s), 330 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250116_193351-g1e95qqf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ✨ W&B: Initialize a new run to track this model's training\n",
    "wandb.init(project=\"table-quickstart\")\n",
    "\n",
    "# ✨ W&B: Log hyperparameters using config\n",
    "cfg = wandb.config\n",
    "cfg.update({\"epochs\" : EPOCHS, \"batch_size\": BATCH_SIZE, \"lr\" : LEARNING_RATE,\n",
    "            \"l1_size\" : L1_SIZE, \"l2_size\": L2_SIZE,\n",
    "            \"conv_kernel\" : CONV_KERNEL_SIZE,\n",
    "            \"img_count\" : min(10000, NUM_IMAGES_PER_BATCH*NUM_BATCHES_TO_LOG)})\n",
    "\n",
    "# define model, loss, and optimizer\n",
    "model = ConvNet(NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# convenience funtion to log predictions for a batch of test images\n",
    "def log_test_predictions(images, labels, outputs, predicted, test_table, log_counter):\n",
    "  # obtain confidence scores for all classes\n",
    "  scores = F.softmax(outputs.data, dim=1)\n",
    "  log_scores = scores.cpu().numpy()\n",
    "  log_images = images.cpu().numpy()\n",
    "  log_labels = labels.cpu().numpy()\n",
    "  log_preds = predicted.cpu().numpy()\n",
    "  # adding ids based on the order of the images\n",
    "  _id = 0\n",
    "  for i, l, p, s in zip(log_images, log_labels, log_preds, log_scores):\n",
    "    # add required info to data table:\n",
    "    # id, image pixels, model's guess, true label, scores for all classes\n",
    "    img_id = str(_id) + \"_\" + str(log_counter)\n",
    "    test_table.add_data(img_id, wandb.Image(i), p, l, *s)\n",
    "    _id += 1\n",
    "    if _id == NUM_IMAGES_PER_BATCH:\n",
    "      break\n",
    "\n",
    "# train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(EPOCHS):\n",
    "    # training step\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ✨ W&B: Log loss over training steps, visualized in the UI live\n",
    "        wandb.log({\"loss\" : loss})\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                .format(epoch+1, EPOCHS, i+1, total_step, loss.item()))\n",
    "\n",
    "\n",
    "    # ✨ W&B: Create a Table to store predictions for each test step\n",
    "    columns=[\"id\", \"image\", \"guess\", \"truth\"]\n",
    "    for digit in range(10):\n",
    "      columns.append(\"score_\" + str(digit))\n",
    "    test_table = wandb.Table(columns=columns)\n",
    "\n",
    "    # test the model\n",
    "    model.eval()\n",
    "    log_counter = 0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            if log_counter < NUM_BATCHES_TO_LOG:\n",
    "              log_test_predictions(images, labels, outputs, predicted, test_table, log_counter)\n",
    "              log_counter += 1\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        acc = 100 * correct / total\n",
    "        # ✨ W&B: Log accuracy across training epochs, to visualize in the UI\n",
    "        wandb.log({\"epoch\" : epoch, \"acc\" : acc})\n",
    "        print('Test Accuracy of the model on the 10000 test images: {} %'.format(acc))\n",
    "\n",
    "    # ✨ W&B: Log predictions table to wandb\n",
    "    wandb.log({\"test_predictions\" : test_table})\n",
    "\n",
    "# ✨ W&B: Mark the run as complete (useful for multi-cell notebook)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
